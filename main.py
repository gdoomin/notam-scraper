import os
import time
import re
import shutil
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from supabase import create_client, Client

def extract_coords(full_text):
    try:
        match = re.search(r'(\d{4}[NS])(\d{5}[EW])', full_text)
        if match:
            lat_str, lng_str = match.groups()
            lat = int(lat_str[:2]) + int(lat_str[2:4])/60
            if 'S' in lat_str: lat = -lat
            lng = int(lng_str[:3]) + int(lng_str[3:5])/60
            if 'W' in lng_str: lng = -lng
            return lat, lng
    except: pass
    return 37.5665, 126.9780

def run_scraper():
    # 1. Supabase ì„¤ì •
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    supabase: Client = create_client(url, key)

    # 2. ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    download_dir = os.path.join(os.getcwd(), "downloads")
    if os.path.exists(download_dir):
        shutil.rmtree(download_dir)
    os.makedirs(download_dir)

    # 3. ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (íƒ€ì„ì•„ì›ƒ ë°©ì–´)
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # í˜ì´ì§€ ë¡œë“œ ì „ëµ ì„¤ì • (DOM êµ¬ì„±ê¹Œì§€ë§Œ ê¸°ë‹¤ë¦¼)
    options.page_load_strategy = 'eager'
    
    # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨í•˜ì—¬ ì†ë„ í–¥ìƒ
    prefs = {
        "download.default_directory": download_dir,
        "safebrowsing.enabled": True,
        "profile.managed_default_content_settings.images": 2 
    }
    options.add_experimental_option("prefs", prefs)
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    # ë“œë¼ì´ë²„ ìˆ˜ì¤€ íƒ€ì„ì•„ì›ƒ í™•ì¥ (3ë¶„)
    driver.set_page_load_timeout(180)
    driver.set_script_timeout(180)
    wait = WebDriverWait(driver, 40)

    try:
        print("ğŸŒ KOCA í˜ì´ì§€ ì ‘ì† ì¤‘ (íƒ€ì„ì•„ì›ƒ ëŒ€í­ í™•ì¥)...")
        driver.get("https://aim.koca.go.kr/xNotam/index.do?type=search2&language=ko_KR")
        time.sleep(30) # JS ì™„ì „ ë¡œë”© ëŒ€ê¸°

        # 4. ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        try:
            page_elements = driver.find_elements(By.CSS_SELECTOR, ".pagination a, .paging a, .page_num a")
            page_numbers = [int(el.text) for el in page_elements if el.text.strip().isdigit()]
            total_pages = max(page_numbers) if page_numbers else 1
        except:
            total_pages = 1
        
        print(f"ğŸ“Š ì´ {total_pages}ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘")

        for p in range(1, total_pages + 1):
            print(f"ğŸ“„ {p} / {total_pages} í˜ì´ì§€ ì‘ì—… ì¤‘...")
            
            if p > 1:
                # ë‹¤ìŒ í˜ì´ì§€ í´ë¦­ (ìˆ«ì í…ìŠ¤íŠ¸ ê¸°ì¤€)
                page_btn = wait.until(EC.element_to_be_clickable((By.XPATH, f"//a[text()='{p}']")))
                driver.execute_script("arguments[0].click();", page_btn)
                time.sleep(12) # í˜ì´ì§€ ì „í™˜ ë° í…Œì´ë¸” ê°±ì‹  ëŒ€ê¸°

            # 5. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ì •ë°€ XPath ì‚¬ìš©)
            target_xpath = '//*[@id="realContents"]/div[3]/div[1]/div/div/a[3]'
            excel_btn = wait.until(EC.presence_of_element_located((By.XPATH, target_xpath)))
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", excel_btn)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", excel_btn)
            
            # ë‹¤ìš´ë¡œë“œ ê°„ê²© ìœ ì§€
            time.sleep(15)

        print("â³ ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ëŒ€ê¸° ì¤‘...")
        time.sleep(10)

        # 6. ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ë³‘í•©
        files = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.endswith(('.xls', '.xlsx'))]
        if not files:
            print("ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“‚ ìˆ˜ì§‘ëœ {len(files)}ê°œ íŒŒì¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±° ì¤‘...")
        all_dfs = []
        for f in files:
            try:
                # xlrdëŠ” .xls íŒŒì¼ì„ ì½ì„ ë•Œ í•„ìš”í•©ë‹ˆë‹¤.
                all_dfs.append(pd.read_excel(f, engine='xlrd'))
            except: continue

        if not all_dfs: return
        
        df = pd.concat(all_dfs, ignore_index=True)
        df.drop_duplicates(subset=['Notam#'], keep='first', inplace=True)
        print(f"âœ… ìœ íš¨ ë…¸íƒ ë°ì´í„° {len(df)}ê±´ í™•ë³´")

        # 7. ë°ì´í„° ê°€ê³µ ë° Supabase ì €ì¥
        notam_list = []
        for _, row in df.iterrows():
            notam_id = str(row.get('Notam#', ''))
            full_text = str(row.get('Full Text', ''))
            lat, lng = extract_coords(full_text)
            
            notam_list.append({
                "notam_id": notam_id,
                "content": full_text,
                "lat": lat,
                "lng": lng,
                "series": notam_id[0] if notam_id else "U",
                "start_date": str(row.get('Start Date UTC', '')),
                "end_date": str(row.get('End Date UTC', ''))
            })

        if notam_list:
            supabase.table("notams").upsert(notam_list, on_conflict="notam_id").execute()
            print(f"ğŸš€ ìµœì¢… ì„±ê³µ: {len(notam_list)}ê°œì˜ ì „ì²´ ë…¸íƒì´ Supabaseì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ğŸš¨ ëŸ°íƒ€ì„ ì—ëŸ¬: {e}")
        driver.save_screenshot("timeout_debug.png")
    finally:
        driver.quit()

if __name__ == "__main__":
    run_scraper()
