import os
import time
import re
import shutil
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from supabase import create_client, Client

# 1. ì¢Œí‘œ ì¶”ì¶œ í•¨ìˆ˜ (Doo GPX ì§€ë„ í‘œì‹œìš©)
def extract_coords(full_text):
    try:
        match = re.search(r'(\d{4}[NS])(\d{5}[EW])', full_text)
        if match:
            lat_str, lng_str = match.groups()
            lat = int(lat_str[:2]) + int(lat_str[2:4])/60
            if 'S' in lat_str: lat = -lat
            lng = int(lng_str[:3]) + int(lng_str[3:5])/60
            if 'W' in lng_str: lng = -lng
            return lat, lng
    except: pass
    return 37.5665, 126.9780

def run_scraper():
    # 2. Supabase ë° í™˜ê²½ ì„¤ì •
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    supabase: Client = create_client(url, key)

    download_dir = os.path.join(os.getcwd(), "downloads")
    if os.path.exists(download_dir):
        shutil.rmtree(download_dir)
    os.makedirs(download_dir)

    # 3. ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (ë³´ì•ˆ ì •ì±… í•´ì œ ë° ìµœì í™”)
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    options.page_load_strategy = 'eager'
    
    # í•µì‹¬ ìˆ˜ì •: ë‹¤ì¤‘ ë‹¤ìš´ë¡œë“œ ìë™ í—ˆìš© ë° ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨
    prefs = {
        "download.default_directory": download_dir,
        "safebrowsing.enabled": True,
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.multiple_automatic_downloads": 1 
    }
    options.add_experimental_option("prefs", prefs)
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.set_page_load_timeout(180)
    wait = WebDriverWait(driver, 45)

    try:
        print(f"ğŸŒ KOCA ì ‘ì† ì‹œì‘: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        driver.get("https://aim.koca.go.kr/xNotam/index.do?type=search2&language=ko_KR")
        time.sleep(30) # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°

        print("ğŸ“Š ë©€í‹° í˜ì´ì§€ ìˆ˜ì§‘ ê°€ë™ (ë‹¤ì¤‘ ë‹¤ìš´ë¡œë“œ ê¶Œí•œ íšë“)...")
        
        for p in range(1, 11): 
            print(f"ğŸ“„ {p}í˜ì´ì§€ ì‘ì—… ì‹œë„ ì¤‘...")
            
            # --- í˜ì´ì§€ ì´ë™ (2í˜ì´ì§€ë¶€í„°) ---
            if p > 1:
                try:
                    td_idx = p + 3 # 2í˜ì´ì§€=td[5] ê·œì¹™
                    # ì£¼ì‹  ì ˆëŒ€ ê²½ë¡œ XPath í™œìš©
                    page_xpath = f'/html/body/div[2]/div[3]/div[2]/div/div/div[2]/div[3]/div[2]/div/div/div/div/div/table/tbody/tr[5]/td/div/table/tbody/tr/td[{td_idx}]'
                    
                    page_btn = wait.until(EC.element_to_be_clickable((By.XPATH, page_xpath)))
                    driver.execute_script("arguments[0].click();", page_btn)
                    print(f"   -> {p}í˜ì´ì§€ ì´ë™ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                    time.sleep(20) # ì„œë²„ì—ì„œ ì—‘ì…€ ìƒì„± ë°ì´í„°ë¥¼ ì¤€ë¹„í•  ì‹œê°„ì„ ì¶©ë¶„íˆ ì¤Œ
                except:
                    print(f"   -> ë” ì´ìƒì˜ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. íƒìƒ‰ ì¢…ë£Œ.")
                    break

            # --- ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í´ë¦­ ---
            try:
                excel_xpath = '//*[@id="realContents"]/div[3]/div[1]/div/div/a[3]'
                # í˜ì´ì§€ê°€ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ìš”ì†Œë¥¼ ìƒˆë¡œ ì°¾ìŠµë‹ˆë‹¤.
                excel_btn = wait.until(EC.presence_of_element_located((By.XPATH, excel_xpath)))
                driver.execute_script("arguments[0].click();", excel_btn)
                print(f"   -> {p}í˜ì´ì§€ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì™„ë£Œ")
                
                # --- íŒŒì¼ ì´ë¦„ ì¦‰ì‹œ ë³€ê²½ (ì¶©ëŒ ë° ìœ ì‹¤ ë°©ì§€) ---
                renamed = False
                for i in range(60): # ê¹ƒí—ˆë¸Œ ì•¡ì…˜ ì†ë„ ê³ ë ¤ (ìµœëŒ€ 60ì´ˆ)
                    time.sleep(1)
                    # page_ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ì‹¤ì œ ì—‘ì…€ íŒŒì¼ ì°¾ê¸°
                    new_files = [f for f in os.listdir(download_dir) 
                                 if not f.startswith('page_') and not f.endswith('.crdownload') and f.endswith(('.xls', '.xlsx'))]
                    
                    if new_files:
                        time.sleep(3) # íŒŒì¼ ì“°ê¸° ì™„ë£Œë¥¼ ìœ„í•œ ì•ˆì „ ëŒ€ê¸°
                        old_path = os.path.join(download_dir, new_files[0])
                        new_filename = f"page_{p}_notam.xls"
                        new_path = os.path.join(download_dir, new_filename)
                        os.rename(old_path, new_path)
                        print(f"   -> [í™•ë³´ ì„±ê³µ] {new_filename}")
                        renamed = True
                        break
                
                if not renamed:
                    print(f"   âš ï¸ {p}í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ê°ì§€ ì‹¤íŒ¨ (ë¸Œë¼ìš°ì € ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸ í•„ìš”)")
                    
            except Exception as e:
                print(f"   âš ï¸ {p}í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

        # 4. ëª¨ë“  ê°œë³„ íŒŒì¼ ë³‘í•© ë¡œì§
        all_files = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.startswith('page_')]
        print(f"ğŸ“‚ ì´ {len(all_files)}ê°œì˜ íŒŒì¼ì„ ë³‘í•©í•©ë‹ˆë‹¤...")
        
        if len(all_files) == 0:
            print("ğŸš¨ ìˆ˜ì§‘ëœ íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
            return

        all_dfs = []
        for f in all_files:
            try:
                temp_df = pd.read_excel(f, engine='xlrd')
                all_dfs.append(temp_df)
                print(f"   -> {os.path.basename(f)} ì½ê¸° ì™„ë£Œ: {len(temp_df)}í–‰")
            except Exception as e:
                print(f"   âš ï¸ {f} ì½ê¸° ì˜¤ë¥˜: {e}")

        # 5. ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì—…ë¡œë“œ ë°ì´í„° ìƒì„±
        full_df = pd.concat(all_dfs, ignore_index=True)
        full_df.drop_duplicates(subset=['Notam#'], keep='first', inplace=True)
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ìœ íš¨ ë°ì´í„°: {len(full_df)}ê±´")

        notam_list = []
        for _, row in full_df.iterrows():
            notam_id = str(row.get('Notam#', ''))
            full_text = str(row.get('Full Text', ''))
            lat, lng = extract_coords(full_text)
            
            notam_list.append({
                "notam_id": notam_id,
                "content": full_text,
                "lat": lat,
                "lng": lng,
                "series": notam_id[0] if notam_id else "U",
                "start_date": str(row.get('Start Date UTC', '')),
                "end_date": str(row.get('End Date UTC', ''))
            })

        if notam_list:
            # Supabaseì— 346ê±´ ì´ìƒì˜ ì „ì²´ ë°ì´í„° í•œ ë²ˆì— ì—…ë¡œë“œ
            supabase.table("notams").upsert(notam_list, on_conflict="notam_id").execute()
            print(f"ğŸš€ [ì¶•í•˜í•©ë‹ˆë‹¤!] ì´ {len(notam_list)}ê±´ì˜ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: {e}")
        driver.save_screenshot("scraper_error.png")
    finally:
        driver.quit()

if __name__ == "__main__":
    run_scraper()
