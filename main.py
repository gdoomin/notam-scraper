import os
import time
import re
import shutil
import glob
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from supabase import create_client, Client

def extract_coords(full_text):
    try:
        match = re.search(r'(\d{4}[NS])(\d{5}[EW])', full_text)
        if match:
            lat_str, lng_str = match.groups()
            lat = int(lat_str[:2]) + int(lat_str[2:4])/60
            if 'S' in lat_str: lat = -lat
            lng = int(lng_str[:3]) + int(lng_str[3:5])/60
            if 'W' in lng_str: lng = -lng
            return lat, lng
    except: pass
    return 37.5665, 126.9780

def wait_for_new_download(download_dir, existing_files, timeout=90):
    """ê¸°ì¡´ íŒŒì¼ ëª©ë¡ê³¼ ë¹„êµí•´ì„œ ìƒˆ íŒŒì¼ì´ ì™„ì „íˆ ë‹¤ìš´ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    for _ in range(timeout):
        time.sleep(1)
        current_files = set(os.listdir(download_dir))
        new_files = [f for f in current_files - existing_files 
                     if not f.endswith('.crdownload') and not f.endswith('.tmp')]
        if new_files:
            time.sleep(2)  # íŒŒì¼ ì“°ê¸° ì™„ë£Œ ëŒ€ê¸°
            return new_files[0]
    return None

def click_page(driver, wait, page_num):
    """í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ - ì—¬ëŸ¬ ë°©ì‹ ì‹œë„"""
    try:
        # ë°©ë²• 1: í…ìŠ¤íŠ¸ë¡œ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
        page_btns = driver.find_elements(By.XPATH, 
            f"//table//td[normalize-space(text())='{page_num}']")
        for btn in page_btns:
            if btn.is_displayed():
                driver.execute_script("arguments[0].click();", btn)
                print(f"   -> {page_num}í˜ì´ì§€ ë²„íŠ¼ í´ë¦­ (í…ìŠ¤íŠ¸ ë°©ì‹)")
                return True
    except: pass
    
    try:
        # ë°©ë²• 2: a íƒœê·¸ë¡œ ì°¾ê¸°
        page_links = driver.find_elements(By.XPATH,
            f"//a[normalize-space(text())='{page_num}']")
        for link in page_links:
            if link.is_displayed():
                driver.execute_script("arguments[0].click();", link)
                print(f"   -> {page_num}í˜ì´ì§€ ë§í¬ í´ë¦­")
                return True
    except: pass
    
    return False

def run_scraper():
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    supabase: Client = create_client(url, key)

    download_dir = os.path.join(os.getcwd(), "downloads")
    if os.path.exists(download_dir):
        shutil.rmtree(download_dir)
    os.makedirs(download_dir)

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    
    prefs = {
        "download.default_directory": download_dir,
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
    }
    options.add_experimental_option("prefs", prefs)
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.execute_cdp_cmd('Page.setDownloadBehavior', {
        'behavior': 'allow',
        'downloadPath': download_dir
    })
    driver.set_page_load_timeout(180)
    wait = WebDriverWait(driver, 60)

    try:
        print(f"ğŸŒ KOCA ì ‘ì†: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        driver.get("https://aim.koca.go.kr/xNotam/index.do?type=search2&language=ko_KR")
        
        print("â³ ì´ˆê¸° ë¡œë”© ëŒ€ê¸° (40ì´ˆ)...")
        time.sleep(40)

        for p in range(1, 5):  # 1~4í˜ì´ì§€
            print(f"\n{'='*40}")
            print(f"ğŸ“„ {p}í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘")
            
            # 1í˜ì´ì§€ ì œì™¸í•˜ê³  í˜ì´ì§€ ì´ë™
            if p > 1:
                success = click_page(driver, wait, p)
                if not success:
                    print(f"   âš ï¸ {p}í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•¨. í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡° í™•ì¸ í•„ìš”")
                    # ë””ë²„ê¹…: í˜„ì¬ í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ HTML ì¶œë ¥
                    try:
                        pagination = driver.find_element(By.XPATH, "//table[.//td[@class='paginate_button']]")
                        print(f"   [DEBUG] í˜ì´ì§€ë„¤ì´ì…˜ HTML:\n{pagination.get_attribute('outerHTML')[:500]}")
                    except:
                        print("   [DEBUG] í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                print(f"   â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°...")
                time.sleep(25)

            # í˜„ì¬ ë‹¤ìš´ë¡œë“œ í´ë” ìƒíƒœ ìŠ¤ëƒ…ìƒ·
            existing_files = set(os.listdir(download_dir))
            
            # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­
            try:
                # ì—‘ì…€ ë²„íŠ¼ - ì—¬ëŸ¬ XPath/ì„ íƒì ì‹œë„
                excel_btn = None
                selectors = [
                    '//*[@id="realContents"]/div[3]/div[1]/div/div/a[3]',
                    '//a[contains(@onclick, "excel") or contains(@href, "excel")]',
                    '//a[contains(text(), "ì—‘ì…€") or contains(text(), "Excel") or contains(text(), "XLS")]',
                    '//img[contains(@src, "excel") or contains(@alt, "excel")]/parent::a',
                ]
                
                for sel in selectors:
                    try:
                        el = driver.find_element(By.XPATH, sel)
                        if el.is_displayed():
                            excel_btn = el
                            print(f"   -> ì—‘ì…€ ë²„íŠ¼ ë°œê²¬: {sel[:50]}")
                            break
                    except: continue
                
                if not excel_btn:
                    print(f"   âš ï¸ {p}í˜ì´ì§€ ì—‘ì…€ ë²„íŠ¼ ì—†ìŒ")
                    # ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦°ìƒ·
                    driver.save_screenshot(f"debug_page_{p}.png")
                    print(f"   [DEBUG] ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_page_{p}.png")
                    continue
                
                driver.execute_script("arguments[0].click();", excel_btn)
                print(f"   -> ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í´ë¦­")
                
            except Exception as e:
                print(f"   âš ï¸ ì—‘ì…€ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                continue

            # ìƒˆ íŒŒì¼ ëŒ€ê¸°
            print(f"   â³ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°...")
            new_file = wait_for_new_download(download_dir, existing_files, timeout=90)
            
            if new_file:
                old_path = os.path.join(download_dir, new_file)
                new_filename = f"page_{p}_notam.xls"
                new_path = os.path.join(download_dir, new_filename)
                # ê¸°ì¡´ ë™ëª… íŒŒì¼ ìˆìœ¼ë©´ ì‚­ì œ
                if os.path.exists(new_path):
                    os.remove(new_path)
                os.rename(old_path, new_path)
                file_size = os.path.getsize(new_path)
                print(f"   âœ… [{p}í˜ì´ì§€] {new_filename} ì €ì¥ ì™„ë£Œ ({file_size:,} bytes)")
            else:
                print(f"   âš ï¸ [{p}í˜ì´ì§€] ë‹¤ìš´ë¡œë“œ ê°ì§€ ì‹¤íŒ¨")
                driver.save_screenshot(f"debug_download_fail_p{p}.png")

        # íŒŒì¼ ë³‘í•©
        all_files = sorted(glob.glob(os.path.join(download_dir, 'page_*.xls')))
        print(f"\n{'='*40}")
        print(f"ğŸ“‚ ì´ {len(all_files)}ê°œ íŒŒì¼ ë³‘í•© ì¤‘...")

        if not all_files:
            print("ğŸš¨ íŒŒì¼ ì—†ìŒ. debug_*.png ìŠ¤í¬ë¦°ìƒ· í™•ì¸í•˜ì„¸ìš”.")
            return

        all_dfs = []
        for f in all_files:
            try:
                df = pd.read_excel(f, engine='xlrd')
                print(f"   -> {os.path.basename(f)}: {len(df)}ê±´")
                all_dfs.append(df)
            except Exception as e:
                print(f"   âš ï¸ {f} ì½ê¸° ì‹¤íŒ¨: {e}")

        full_df = pd.concat(all_dfs, ignore_index=True)
        full_df.drop_duplicates(subset=['Notam#'], keep='first', inplace=True)
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„ ìµœì¢…: {len(full_df)}ê±´")

        notam_list = []
        for _, row in full_df.iterrows():
            notam_id = str(row.get('Notam#', ''))
            full_text = str(row.get('Full Text', ''))
            lat, lng = extract_coords(full_text)
            notam_list.append({
                "notam_id": notam_id, "content": full_text, "lat": lat, "lng": lng,
                "series": notam_id[0] if notam_id else "U",
                "start_date": str(row.get('Start Date UTC', '')),
                "end_date": str(row.get('End Date UTC', ''))
            })

        if notam_list:
            supabase.table("notams").upsert(notam_list, on_conflict="notam_id").execute()
            print(f"ğŸš€ [ì™„ë£Œ] {len(notam_list)}ê±´ DB ì—…ì„œíŠ¸!")

    except Exception as e:
        import traceback
        print(f"ğŸš¨ ì—ëŸ¬: {e}")
        traceback.print_exc()
        driver.save_screenshot("debug_error.png")
    finally:
        driver.quit()

if __name__ == "__main__":
    run_scraper()
