import os
import time
import re
import shutil
import pandas as pd
from selenium import webdriver
# ... (ìƒë‹¨ import ìƒëµ) ...

def run_scraper():
    # ... (Supabase ì„¤ì • ë° ë¸Œë¼ìš°ì € ì˜µì…˜ ìƒëµ) ...

    try:
        print("ğŸŒ KOCA ì ‘ì† ë° ì´ˆê¸°í™”...")
        driver.get("https://aim.koca.go.kr/xNotam/index.do?type=search2&language=ko_KR")
        time.sleep(30) 

        print("ğŸ“Š ë©€í‹° í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘ (íŒŒì¼ ì´ë¦„ ë³€ê²½ ë¡œì§ ì¶”ê°€)...")
        
        for p in range(1, 11): 
            print(f"ğŸ“„ {p}í˜ì´ì§€ ì‘ì—… ì¤‘...")
            
            if p > 1:
                try:
                    td_idx = p + 3 
                    page_xpath = f'//*[@id="notamSheet-table"]/tbody/tr[5]/td/div/table/tbody/tr/td[{td_idx}]'
                    page_btn = wait.until(EC.element_to_be_clickable((By.XPATH, page_xpath)))
                    driver.execute_script("arguments[0].click();", page_btn)
                    print(f"   -> {p}í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                    time.sleep(15) 
                except:
                    print(f"   -> {p}í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ (ì¢…ë£Œ)")
                    break

            # 1. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í´ë¦­
            excel_xpath = '//*[@id="realContents"]/div[3]/div[1]/div/div/a[3]'
            excel_btn = wait.until(EC.presence_of_element_located((By.XPATH, excel_xpath)))
            driver.execute_script("arguments[0].click();", excel_btn)
            print(f"   -> {p}í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ ìš”ì²­")
            
            # 2. íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ê°ì‹œ ë° ì´ë¦„ ë³€ê²½ (í•µì‹¬!)
            downloaded = False
            for _ in range(30): # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
                time.sleep(1)
                files = [f for f in os.listdir(download_dir) if not f.endswith('.crdownload')]
                if files:
                    # ë°©ê¸ˆ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ 'page_p.xls' í˜•íƒœë¡œ ë³€ê²½
                    for f in files:
                        if not f.startswith("page_"):
                            old_path = os.path.join(download_dir, f)
                            new_path = os.path.join(download_dir, f"page_{p}_{f}")
                            os.rename(old_path, new_path)
                            print(f"   -> íŒŒì¼ ì €ì¥ ì™„ë£Œ: page_{p}_{f}")
                            downloaded = True
                            break
                if downloaded: break
            
        # 3. ëª¨ë“  íŒŒì¼ ë³‘í•©
        files = [os.path.join(download_dir, f) for f in os.listdir(download_dir) if f.startswith("page_")]
        print(f"ğŸ“‚ ì´ {len(files)}ê°œ íŒŒì¼ í†µí•© ì‹œì‘...")
        
        all_dfs = []
        for f in files:
            try:
                all_dfs.append(pd.read_excel(f, engine='xlrd'))
            except Exception as e:
                print(f"âš ï¸ {f} ì½ê¸° ì‹¤íŒ¨: {e}")

        if not all_dfs: return
        
        full_df = pd.concat(all_dfs, ignore_index=True)
        full_df.drop_duplicates(subset=['Notam#'], keep='first', inplace=True)
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„ ìµœì¢… {len(full_df)}ê±´ í™•ë³´!")

        # ... (ì´í›„ Supabase ì—…ë¡œë“œ ë¡œì§ ë™ì¼) ...

        notam_list = []
        for _, row in full_df.iterrows():
            notam_id = str(row.get('Notam#', ''))
            full_text = str(row.get('Full Text', ''))
            lat, lng = extract_coords(full_text)
            notam_list.append({
                "notam_id": notam_id, "content": full_text, "lat": lat, "lng": lng,
                "series": notam_id[0] if notam_id else "U",
                "start_date": str(row.get('Start Date UTC', '')),
                "end_date": str(row.get('End Date UTC', ''))
            })

        if notam_list:
            supabase.table("notams").upsert(notam_list, on_conflict="notam_id").execute()
            print(f"ğŸš€ [ì„±ê³µ] ì´ {len(notam_list)}ê°œì˜ ë…¸íƒì´ Supabaseì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"ğŸš¨ ì¹˜ëª…ì  ì—ëŸ¬: {e}")
        driver.save_screenshot("pagination_error.png")
    finally:
        driver.quit()

if __name__ == "__main__":
    run_scraper()
